\section{Resampling methods}
\subsection{Systematic Resampling}
Systematic resampling is widely used resampling method for particle filters. It is preferred because it is computationally simple and have good empirical performance \cite{douc2005comparison}. The systematic resampling method have shown to be empirically comparable with other resampling methods such as stratified sampling and residual resampling which in turn have been shown to be better than multinomial resampling \cite{douc2005comparison}.

In practice it is implemented as follows:
\begin{algorithm}[H]
	\begin{algorithmic}
		\State Draw: $r \sim U(0,1) $
		\For{$i=0:M-1$}
			\State $U^i \gets (i+r)/M$
			\State $I^i \gets D^{inv}_w(U_i)$
		\EndFor
	\end{algorithmic}
	\caption{Systematic Resampling algorithm}
\end{algorithm}
Where $D^{inv}_w$ is the inverse of the cumulative distribution function associated with the particle weights $\{w_t^i\}_{i=1}^{N}$, $M$ is the number of samples to draw, and $I^i$ is the index of the $i$'th sample. This resampling method is sensitive to the order of the particles before the resampling as it changes the cumulative distribution function.

\subsection{Cluster Resampling}
The idea behind cluster resampling is that we are not interested in particles, we are interested in the high probability areas the particles represent. One of the main problems of systematic resampling is that sampling variance can eliminate all particles in one area (figure \ref{fig:sysresamp}). Cluster resampling remedies this problem by representing areas as a cluster/mixture of particles and then resample particles from one cluster at the time. The number of particles resampled from one cluster is proportional to the weight of the cluster, thus if an cluster have 10\% of the weight, we draw 10\% of the particles from that cluster (figure \ref{fig:mixresamp}).

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{2.5in}
		\fbox{\includegraphics[width=\textwidth]{figs/cluster_demo1}}
		\caption{Standard resampling from this distribution can eliminate the all orange particles due to sampling variance since there is nothing preventing the resampler from only selecting blue particles.}
		\label{fig:sysresamp}
	\end{subfigure}
	~
	\begin{subfigure}[t]{2.5in}
		\fbox{\includegraphics[width=\textwidth]{figs/cluster_demo2}}
		\caption{By clustering particles and thereafter resampling 10\% of the particles from the orange cluster and 90\% from the blue cluster, there will be exactly 10 orange and 90 blue particles in the new set.}
		\label{fig:mixresamp}
	\end{subfigure}
	\caption{100 hundred particles to be resampled. 90\% of the weight is in the blue particles and 10\% is in the orange particles.}
\end{figure}
\subsubsection{Algorithm}
The generic algorithm for ones step of cluster resampling is the following:
\begin{enumerate}[label=\textbf{step \arabic*:}]
	\setlength{\itemindent}{.25in}
	\item Propagate particles
	
	\item Weight update
	
	\item Resample particles
	
	\item Calculate particles clusters
\end{enumerate}
where the step 3 and 4 can exchange place.
\subsubsection{Mixture Representation}
In cluster resampling, we represent the target distribution as an $M$-component mixture model. Let $z_t$ denote the state at time $t$ and let $y^t =\{ y_1,y_2,\cdots, y_t\}$ be the observations. Then the target distribution $p(z_t|y^t)$ is the following,
\begin{equation}
	p(z_t|y^t) = \sum_{m=1}^{M}\pi_{m,t}p_m(x_t|y^t)
	\label{eq:mix}
\end{equation}
where $\sum_{m=1}^{M} \pi_{m,t} = 1$.

The particle filter approximation of the mixture model represents each mixture as a cluster of particles. Let the particles $\{z_t^i\}_{n=1}^N$ be clustered into $M$ different clusters $\calI_{m,t}$ representing the mixture components $m$. Let each particle $z_t^i$ belong to one and only one cluster $m$ and let $c_{t,i} = m$. The mixture components $p_m(z_t|y^t)$ are approximated by:
\begin{equation}
	q_m(z_t|y^t) = \sum_{i \in \calI_{m,t}} w_t^i\delta_{z_t^i}(z_t)
	\label{eq:clustcomp}
\end{equation}

Inserting (\ref{eq:clustcomp}) into (\ref{eq:mix}) gives us the following approximation of $p(z_t|y^{t-1})$
\begin{gather}
	q(z_t|y^{t-1}) = \sum_{m=1}^{M}\pi_{m,t}\sum_{i \in \calI_{m,t}}w^i_t\delta_{z_t^i}(z_t)
\end{gather}
where the weights $\pi_{m,t}$ and $w_t^i$ are computed as follows:
\begin{gather}
	w_t^i = \frac{\hat{w}_t^i}{\sum_{i\in \calI_{m,t}} \hat{w}_t^i}, ~\hat{w}_t^i = p(y_t|z_t^i)w_{t-1}^i\\
	\pi_{m,t} = \frac{\pi_{m,t}w_{m,t}}{\sum_{m'=1}^{M}\pi_{m',t}w_{m',t}}, ~w_{m,t} = \sum_{i\in \calI_{m,t}} \hat{w}_t^i.
\end{gather}
It can be shown that this approximation is identical to the approximation used in normal particle filtering. For a complete derivation, see \cite{vermaak2003maintaining}.

\subsubsection{Resampling step}
The main difference between standard resampling methods (e.g. systematic resampling), and the cluster resampling is that we draw samples from mixture components. When drawing samples from one mixture, we can use standard resampling methods such as systematic resampling. When drawing $n$ samples from a mixture component $m$, it is important to note that the samples are drawn independently of other components, thus the new particles weights are $\frac{1}{n}$.
\subsubsection{Clustering step}
Clustering is the task to group objects in such a way that objects in the same group are more similar to each other than other groups. The algorithms we are interested in are two versions of the k-means clustering algorithms, Lloyd's algorithm and the EM algorithm.

\paragraph{Lloyd's algorithm}Given an initial set of $M$ centroids $m_1, m_2, \dots, m_M$, the k-means algorithm proceeds in two steps:
\begin{description}
	\setlength{\itemindent}{0.2in}
	\item[Assignment:] Every object $z^i$ is assigned to the nearest centroid $k$. The distance is measured with the euclidean distance. 
	\item[Update:] New centroids $m'_k$ are calculated by taking the mean of each object $z^i$ that belongs to $k$; that is, $m'_k = \frac{1}{|\calI_k|}\sum_{i \in \calI_k}z^i$,
	where $\calI_k$ is the set of objects assigned to centroid $k$.
\end{description}
this is done iteratively until the assignments no longer changes.

\paragraph{EM algorithm} The algorithm starts with an initial initial set of mean vectors $m_1, m_2, \dots, m_M$. Then the EM algorithm proceeds in 2 steps:
\begin{description}
	\setlength{\itemindent}{0.2in}
	\item[Expectation:] Every object $i$ is partially assigned to each mixture $k$, this is represented by a weight 
	$w_k^i = \frac{p_k(z^i)}{\sum_{k'=1}^{M}p_{k'}(z^i)}$
	where $p_k(z^i) = \calN(z^i|m_k, \sigma^2I)$.
	\item[Maximization:] New means $m'_k$ are calculated by taking the weighted mean of each object $z^i$; that is, $m'_k = \frac{\sum_{i \in \calI_k}w_k^iz^i}{\sum_{i \in \calI_k}w_k^i}$.
\end{description}
this is repeated until convergence. The EM algorithm for clustering is based on the EM algorithm for GMM.

Both algorithms suffers from local minima; that is, the optimal clustering is not found. Because of the local minima problem, the algorithms may need to be run several times to find a better clustering. They algorithms are also biased towards spherical or hyper-spherical clusters. The EM algorithm is in general better than Lloyd's algorithm since it has fewer local minimas, however, Lloyd's algorithm is easier to implement and faster.

\subsection{Parallel Particle Filter}

